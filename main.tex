% LaTeX Notetaking Template for the 2019 Pan-Canadian Self-Organizing Conference on Machine Learning (PC-SOCMLx)
% Template by David Madras, inspired by Meltem Atay's template for the 2018 SOCML

\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{enumitem}
\usepackage{dsfont}
\usepackage[margin=3cm]{geometry}
\usepackage{mathtools}
\usepackage{hyperref}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{float}
\title{ARIMA Notes}
\author{Team Fourier}
\date{November 2020}

\begin{document}

\maketitle
\section{Time Series Data}
Time series data comes in different forms, from prices to temperatures to people as a function of time. Recall one type of time series data relevant to the concept of system control, position over time. In the system identification problem, we have a state transition equation:

\begin{equation}\label{1}
X_{t+1} = AX_{t} + B\mu_{t}
\end{equation}

What is crucial to understand is that the only observable output from the system are the series of $X_{t}$'s as well as the inputs placed in to the system, the $\mu$'s. In the system identification problem, we use a series of inputs and outputs to estimate and refine these estimations for the model parameters A and B. \\

We find that this modeling is not always fully applicable to all time series data. For instance, in modeling and forecasting stock prices over time, there is not a well-defined user input, $\mu_{t}$. Also, there may be additional previous system states contributing to the current observation instead of only the immediately previous observation. In these instances, a more appropriate model is sought after to express the behavior of the time series data. \\

In these notes, we will explain how ARIMA and moving average models are more suitable to specific instances of time series data than the basic system identification model, and how the parameters can be estimated through a combination of least squares, used in the traditional system identification process, and iterative maximum likelihood expectation, the pattern used by $k$-means approach to clustering taught in earlier weeks of this course.

\maketitle
\section{Trends}
One commonly encountered time series component is a trend. Trends encompass datasets with non-repetitive generally increasing or decreasing values. An example of a trend is a child's weight as a function of time.
\subsection{Trend Models}
We can model these data by adding a deterministic trend function to white noise:
\begin{equation}\label{1}
X_t=m_t+Z_t
\end{equation}
where $X_t$ is the observation, $m_t$ is the deterministic trend function and $Z_t$ is white noise. Random variables are white noise if they have zero mean and finite variance. This note will cover several methods for fitting \ref{1} to data with trends.
\subsubsection{Linear Regression}
A simple technique would be to perform linear regression on the raw data or a feature augmented data set. If the underlying $m_{t}$ trend is of a polynomial nature, using linear regression can be used to quite accurately fit the time series data. However, if this observation doesn't hold true, using the following moving average techniques can provide a better estimation for each $m_{t}$ using neighboring values.
\subsubsection{Simple Moving Average (SMA)}
SMA averages the previous data points with the current sample to reduce noise. For a $2p$-sample moving average, we calculate:
\begin{equation}\label{2}
    \hat{m}_t=\frac{1}{2p+1}\sum_{i=-p}^p m_{t+i} \footnote{Note in financial applications, the averages are taken over the previous $2n+1$ samples rather than $n$ samples on either side of $m_t$}
\end{equation}
where $\hat{m}_t$ is the \emph{simple moving average} of $X_t$. If $m_t$ is linear over $[t-p, t+p]$, then:
\begin{equation}\label{SMA1}
    \hat{m}_t=m_t + \frac{1}{2p+1}\sum_{i=-p}^p Z_{t+i} \approx m_t
\end{equation}
We can generalize SMA as a weighted sum of the samples:
\begin{equation}\label{SMA2}
    \hat{m}_t=\sum_{i=-p}^p \theta_j X_{t+i}
\end{equation}
where $\theta_j$ are the weights. SMA is a special case of this weighted sum with constant weights of $\frac{1}{2p+1}$. The choice of $p$ affects the bias-variance tradeoff, a property of models that determines sensitivity to noise. Read \href{https://www.machinelearningplus.com/machine-learning/bias-variance-tradeoff/}{bias-variance-tradeoff} for more information on this property. Selecting $p$ often requires a hyperparameter search.
\subsubsection{Exponential Moving Average (EMA)}
EMA follows a similar approach to SMA, but instead of constant weights, EMA heavily emphasizes samples closer in time and depends only on previous observations:
\begin{equation}\label{EMA}
    \hat{m}_t=\frac{1-\phi}{\phi} \sum_{i=1}^\infty \phi^i X_{t-i}
\end{equation}
Here the weights $\phi$ remain constant and samples further back in time are exponentially worth less. We multiply the sum by $\frac{1-\phi}{\phi}$ to ensure the weights sum to 1 since $\sum_{i=1}^\infty \phi^i=\frac{\phi}{1-\phi}$.

\subsection{Detrending Data}
To find patterns in trend models, it is often useful to study the stochastic portions of the models. This way we can exploit structure, if any, and create distributions to sample noise for forecasting. One natural approach to obtain the detrended data is differencing:
\begin{equation}\label{differencing}
    \nabla X_t = X_t - X_{t-1} \qquad \forall t \in 2, \ldots, n.
\end{equation}
Let $m_t=at+b$, a linear trend. Then
\begin{align}\label{differencing simplified}
    \nabla X_t &= a(t+1)+b+Z_{t+1}-(at+b+Z_t) \qquad \forall t \in 2, \ldots, n \nonumber \\
    &=a+Z_{t+1}-Z_t.
\end{align}
If $\nabla X_t$ is white noise, we can simply predict $X_{t+1}$ by forecasting $\nabla X_{t+1}$ as the sample mean $\overline{\nabla X}=\sum_{i=2}^n Y_i/(n-1)$. Then, rearranging (\ref{differencing}) we get:
\begin{align}\label{differencing forecast}
    X_{t+1} &= \nabla X_{t+1} + X_t \qquad \forall t \in 2, \ldots, n \nonumber \\
    &= \overline{\nabla X} + X_t.
\end{align}
Similarly, if there are trends after differencing, we can difference again:
\begin{align}\label{differencing quadratic}
    \nabla^2 X_{t} &= \nabla(\nabla X_t) \qquad \forall t \in 3, \ldots, n \nonumber \\
    &= \nabla X_t - \nabla X_{t-1}\nonumber\\
    &= X_t -2X_{t-1}+X_{t-2}.
\end{align}
Second order differencing removes quadratic trends. A nice way to check understanding is to prove why $\nabla^n$ removes $n^{th}$ order trends.

\section{Stationarity}
Stationary processes form the foundation for systematic time series analysis, especially for \textbf{ARMA} (autoregressive moving average) models. There are two standard definitions of stationarity.
\subsection{Strong or Strict Stationarity}
\textit{A stochastic process $\{X_t\}$ is \textbf{strongly stationary} if the joint distribution of any set of samples $(X_{t_1}, X_{t_2}, \ldots, X_{t_k})$ is the same as the joint distribution of $(X_{t_{1}+\tau}, X_{t_{2} + \tau}, \ldots, X_{t_{k}+\tau}) \forall \tau, t_1, \ldots, t_n \in \mathbb{R}$ and $\forall n \in \mathbb{N}$.}\\\\
This allows us to learn summary statistics of the process, such as means and variances. With this knowledge, we can then predict future observations.
\subsection{Weak or Wide-Sense Stationarity}
\textit{A stochastic process $\{X_t\}$ is \textbf{weakly stationary} if
\begin{enumerate}
    \item $\mathbb{E}[X_t]=\mathbb{E}[X_{t+\tau}] \forall \tau \in \mathbb{R}$
    \item $K_{XX}(t, s)=K_{XX}(t+\tau, s+\tau) \forall \tau \in \mathbb{R}$
\end{enumerate}
where $K_{XX}(t, s)$ is the autocovariance function of $X_t$ evaluated at time $t$ and $s$.}\\\\
Strong and weak stationarities do not imply each other. The rest of this note will refer to \emph{weak stationarity} when discussing \emph{stationarity}.
\subsubsection{Autocovariance Function \emph{(acvf)}}
The \emph{acvf} is defined as:
\begin{align}
    K_{XX}(t, s) &= \text{Cov}[X_t, X_s]\nonumber\\
    &= \mathbb{E}[(X_t-\mu_t)(X_s-\mu_s)]\nonumber\\
    &= \mathbb{E}[X_t, X_s]-\mu_1\mu_2\nonumber
\end{align}
From condition 2. we realize that the covariance of two random variables only only depends on $\tau$, the time lag between them. Therefore, we can further simplify the \emph{acvf}:
\begin{equation}\label{acvf}
    K_{XX}(\tau) = K_{XX}(t, t+\tau)
\end{equation}
We know the covariances of uncorrelated samples:
\begin{equation}
    \text{Cov}(X_t, X_s) = \begin{cases}
        \sigma_z^2\qquad &\text{if } \, t=s\\
        0 \qquad &\text{otherwise.}
    \end{cases}\nonumber
\end{equation}
Likewise, stationary white noise has \emph{acvf}:
\begin{equation}
    K_{XX}(\tau) = \begin{cases}
        \sigma_z^2\qquad &\text{if } \, \tau=0\\
        0 \qquad &\text{otherwise.}
    \end{cases}\nonumber
\end{equation}
\subsubsection{Autocorrelation Function \emph{(acf)}}
The \emph{acf} is defined as:
\begin{equation}\label{acf}
    \rho(t, s) = \frac{K_{XX}(t, s)}{\sqrt{\text{Var}(X_t), \text{Var}(X_s)}}\nonumber
\end{equation}
which can then be simplified to:
\begin{equation}\label{acf}
    \rho(\tau) = \frac{K_{XX}(\tau)}{K_{XX}(0)}.
\end{equation}
The \emph{acf} of a white noise process is:
\begin{equation}
    \rho(\tau) = \begin{cases}
        1\qquad &\text{if } \, \tau=0\\
        0 \qquad &\text{otherwise.}
    \end{cases}\nonumber
\end{equation}
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Stationary.png}
    \caption{Example of stationary process}
    \label{stationary}
\end{figure}
Notice the mean and variance of the data are consistent across time.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{nonstationary.png}
    \caption{Example of nonstationary process}
    \label{nonstatinoary}
\end{figure}
Notice how both the mean and variance of the sequence increase over time.

\section{Moving Average (MA) Models \protect\footnote{Moving average models are different from the moving average (\emph{SMA}).}}
\textit{Let $Z_t, Z_{t-1}, \ldots, Z_{t-q}$ denote a white noise sequence and $\mu$ denote the mean of the sequence. The \textbf{moving average model} of order q, \textbf{MA(q)}, is defined as:
\begin{align}\label{ma model}
    X_t &= \mu + Z_t + \theta_1 Z_{t-1} + \theta_2 Z_{t-2} + \ldots + \theta_q Z_{t-q}\nonumber\\
    &= \mu + \sum_{j=0}^q \theta_j Z_{t-j} \qquad \text{where } \theta_0=1\nonumber\\
    &=  \sum_{j=0}^q \theta_j Z_{t-j}
\end{align}
where $\theta_1, \theta_{2}, \ldots, \theta_{q}$ are the parameters of the model.\\
}
We get (\ref{ma model}) from the second line by assuming a zero mean process. Moving average models allow for forecasting based on past stochastic terms.
\subsection{Weak Stationarity of an MA(q) Model}
\subsubsection{\emph{ACVF} and \emph{ACF}}
The \emph{acvf} is derived as follows:
\begin{align}\label{ma acvf}
    K_{XX}(\tau) &= \text{cov}(\sum_{j=0}^q \theta_j Z_{t-j}, \sum_{k=0}^q \theta_j Z_{t+\tau-k})\nonumber\\
    &= \sum_{j=0}^q \sum_{k=0}^q \theta_j \theta_k \text{cov}(Z_{t-j}, Z_{t+\tau-k})\nonumber
\end{align}
$K_{XX}(h)$ is only non-zero when $t-j=t+\tau-k$, or when $k=j+\tau$, since $Z_t$ are uncorrelated. We know $k$ is bounded by 0 and $q$ which further means $j$ is bounded by 0 and $q-h$. We can rewrite the \emph{acvf} as:
\begin{equation}\label{ma acvf final}
    K_{XX}(h)=\begin{cases}
        \sigma^2\sum_{j=0}^{q-h} \theta_j \theta_{j+h} \qquad &\text{if } \, h \in 0, 1, \ldots, q\\
        0 \qquad &\text{if } h > q
    \end{cases}
\end{equation}
With some pattern matching, we get the \emph{acf}:
\begin{equation}\label{ma acf}
    K_{XX}(h)=\begin{cases}
        \frac{\sum_{j=0}^{q-h} \theta_j \theta_{j+h}}{\sum_{j=0}^q \theta_j^2} \qquad &\text{if } \, h \in 0, 1, \ldots, q\\
        0 \qquad &\text{if } h > q
    \end{cases}
\end{equation}
Notice that $K_{XX}(\tau)$ is independent of $t$. From this we deduce that:\\
\textit{If \{$X_t$\} follows an MA(q) model, then \{$X_t$\} is \textbf{weakly stationary}.}
\subsubsection{Backshift Operator}
Let $B$ denote the backshift operator. It is defined as:
\begin{align}
    BX_t=X_{t-1}, B^2X_t=X_{t-2}, \ldots, B^nX_t=X_{t-n}\nonumber
\end{align}
and
\begin{align}
    B^{-n}X_t=X_{t+n}, \ldots, B^{-1}X_t=X_{t+1}.\nonumber
\end{align}
\subsubsection{Moving Average Operator}
For parameters $\theta_1, \theta_{2}, \ldots, \theta_{q}$ with $\theta_q \neq 0$, define the \textbf{moving average operator} of order q as:
\begin{equation}\label{moving average operator}
    \theta(B) = 1 + \theta_1 B + \ldots + \theta_q B^q.
\end{equation}
This allows us to succinctly write an MA(q) model from (\ref{ma model}) as:
\begin{equation}
    X_t = \theta(B)Z_t.
\end{equation}
\subsection{Invertibility}
Consider an MA(1) model with acf:
\begin{equation}
    \rho(1)=\frac{\theta}{1 + \theta^2}\nonumber
\end{equation}
For any value of $\theta$, $1/\theta$ gives the same autocorrelation. Therefore, there is no unique solution for the parameters of an MA(1) model. To enforce a unique solution, we impose a theoretical restriction to only consider MA(1) models with $\lvert \theta \rvert < 1$. Essentially, $\theta$ must lie within the unit circle. This condition is called \emph{invertibility}. More broadly, \\
\textit{An MA(q) model $X_t=\theta(B)Z_t$ is invertible iff it can be written as
\begin{align}\label{invertible}
    Z_t&=\pi(B)X_t\nonumber\\
    &= \sum_{j=0}^{\infty} \pi_j X_{t-j}
\end{align}
where $\pi(B)=\sum_{j=0}^{\infty} \pi_j B^j$ and $\sum_{j=0}^{\infty} \lvert \pi_j \rvert < \infty$ and $\pi_0=1$.}

\section{Autoregressive (AR) Models}
\textit{Let $Z_t, Z_{t-1}, \ldots, Z_{t-q}$ denote a white noise sequence and $c$ a constant. The \textbf{autoregressive model} of order p, \textbf{AR(p)}, is defined as:
\begin{align}\label{ar model}
    X_t &= c + Z_t + \phi_1 X_{t-1} + \phi_2 X_{t-2} + \ldots + \phi_p X_{t-p}\nonumber\\
    &= c + Z_t + \sum_{j=0}^p \phi_j X_{t-j} \qquad \text{where } \phi_p \neq 0\nonumber\\
    &= Z_t + \sum_{j=0}^p \phi_j X_{t-j} \qquad \text{where } \phi_p \neq 0
\end{align}
where $\phi_1, \ldots, \phi_p$ are parameters of the model and (\ref{ar model}) is a zero mean \textbf{AR(p)} process.\\\\
}
Autoregressive models predict future observations directly using past observations.
\subsection{Autoregressive Operator}
For parameters $\phi_1, \ldots, \phi_p$ with $\phi_p \neq 0$, define the \textbf{autoregressive operator} of order p as:
\begin{equation}\label{ar operator}
    \phi(B)=1-\phi_1B-\ldots \phi_p B^p.
\end{equation}
This allows us to write an AR(p) model from (\ref{ar model}) as:
\begin{equation}
    \phi(B)X_t = Z_t
\end{equation}
which has the polynomial form
\begin{equation}\label{ar poly}
    \phi(z)=1-\phi_1z-\ldots \phi_p z^p.
\end{equation}
\subsection{Causality}
Let us take the case of an AR(1) model:
\begin{equation}\label{ar1}
    X_t = Z_t + \phi X_{t-1}\nonumber
\end{equation}
We can rewrite (\ref{ar1}) as:
\begin{align}\label{ar1 soln}
    X_t &= \frac{Z_t}{\phi(B)}\nonumber\\
    &= \frac{1}{1-\phi B} Z_t
\end{align}
Notice we have the sum of a geometric series for $\lvert \phi \rvert < 1$:
\begin{align}\label{ar less 1}
    X_t &= (1 + \phi B + \phi^2 B^2 + \ldots)Z_t\nonumber\\
    &= \sum_{j=0}^{\infty} \phi^j Z_{t-j}
\end{align}
Now consider $\lvert \phi \rvert > 1$. With some clever algebraic manipulation we can get:
\begin{align}\label{ar greater 1}
    X_t &= (-\frac{B^{-1}}{\phi}-\frac{B^{-2}}{\phi^2}-\ldots)Z_t\nonumber\\
    &= -\frac{Z_{t+1}}{\phi} -\frac{Z_{t+2}}{\phi^2}-\ldots\nonumber\\
    &= - \sum_{j=1}^{\infty}\frac{Z_{t+j}}{\phi^j}.
\end{align}
We see for this case that $X_t$ depends on future stochastic values, which usually does not work for forecasting. For an AR(1) model, a unique stationary solution is causal when $\lvert \phi \rvert < 1$ because it only depends on present and past values of $\{Z_t\}$. An AR(1) model with $\lvert \phi \rvert > 1$ is non-causal because it depends on future values. An exercise to ensure understanding is to show why no stationary solution exists for $\lvert \phi \rvert=1$.\\\\
\textit{We define an AR(p) model as \textbf{causal} if $\phi(z)\neq 0 \text{ for } \lvert z \rvert > 1$.}\\\\
Essentially, $z$ must lie outside the complex unit circle. Equivalently, $\phi$ must lie within the complex unit circle, similar to $\theta$ for an MA model. since $\phi(z)$ can be interpreted as a polynomial (\ref{ar poly}), $z$ plays the role as the roots of the polynomial. If the roots of an AR(p) process are not $1$, the process is stationary. Similar to invertibility for MA(q) models,\\\\
\textit{an AR(p) model is causal iff it can be written as
\begin{align}\label{causal}
    X_t &=\psi(B)Z_t\nonumber\\
    &= \sum_{j=0}^{\infty}\psi_j Z_{t-j}
\end{align}
where $\psi(B)=\sum_{j=0}^{\infty}\psi_j B^j$ and $\sum_{j=0}^{\infty}\lvert \psi_j \rvert < \infty$ and $\psi_0=1$.}
\subsection{Yule-Walker Equations}
Given a zero mean and causal AR(p) process $\{X_t\}$ defined as (\ref{ar model}), we can try to solve for its parameters. Let us begin by multiplying (\ref{ar model}) by $X_{t-1}$ and taking the expectation to obtain:
\begin{align}\label{yule-walker 1}
    \mathbb{E}[X_t X_{t-1}] &= \sum_{j=1}^p \phi_j \mathbb{E} [X_{t-j} X_{t-1}] + \mathbb{E}[Z_t X_{t-1}] \nonumber\\
    &= \sum_{j=1}^p \phi_j \mathbb{E} [X_{t-j} X_{t-1}]
\end{align}
where $\mathbb{E}[Z_t X_{t-1}]$ is $0$ because $Z_t$ is uncorrelated with previous values of the process. We have just derived the Yule-Walker equation for \emph{lag 1}. We repeat this derivation for \emph{lag 2, $\ldots,$ lag p}. As an exercise, derive the Yule-Walker equation for \emph{lag p}. The following is the generalized equation:
\begin{align}\label{yule-walker p}
    \mathbb{E}[X_t X_{t-k}] &= \sum_{j=0}^p \phi_j \mathbb{E} [X_{t-j} X_{t-k}] + \mathbb{E}[Z_t X_{t-k}] \nonumber\\
    &= \sum_{j=0}^p \phi_j \mathbb{E} [X_{t-j} X_{t-k}]
\end{align}
for $k \in [1,\ldots, p]$. Note $r_{-l} = r_l$. Using $r_l$ for $\mathbb{E}[X_{t-l} X_{t-k}]$, (\ref{yule-walker p}) simplifies to:
\begin{equation}\label{yule walker simplified}
    r_l = \sum_{j=1}^p \phi_j r_{j-l}
\end{equation}
We can then reconfigure the $p$ equations into a $p\times p$ matrix multiplication:
\begin{equation}\label{yule-walker matrix}
    \underbrace{\begin{bmatrix}
        r_1\\
        r_2\\
        \vdots\\
        r_{p-1}\\
        r_p
    \end{bmatrix}}_{\textbf{r}}
    =
    \underbrace{\begin{bmatrix}
        r_0 & r_1 &\ldots & r_{p-2} & r_{p-1}\\
        r_1 & r_2 &\ldots & r_{p-3} & r_{p-2}\\
        &  & \vdots & &\\
        r_{p-2} & r_{p-3} & \ldots & r_2 & r_1\\
        r_{p-1} & r_{p-2} & \ldots & r_1 & r_0
    \end{bmatrix}}_{\textbf{R}}
    \underbrace{\begin{bmatrix}
        \phi_1 \\
        \phi_2 \\
        \vdots\\
        \phi_{p-1}\\
        \phi_p
    \end{bmatrix}}_{\textbf{$\Phi$}}
\end{equation}
We can simplify further using compact matrix notation:
\begin{equation}\label{yule walker matrix}
    \textbf{R$\Phi$}=\textbf{r}.
\end{equation}
We know \textbf{R} is full rank and symmetric, so we can invert \textbf{R} to solve for \textbf{$\Phi$}:
\begin{equation}
    \hat{\textbf{$\Phi$}} = \textbf{R$^{-1}$r.}
\end{equation}
\textbf{R} is also known as the \textbf{covariance matrix} of $X$.

\section{ARMA Models}
\textit{Let $Z_t, Z_{t-1}, \ldots, Z_{t-q}$ denote a white noise sequence. The \textbf{AutoRegressive Moving Average model} of order (p, q), \textbf{ARMA(p, q)} is defined as:
\begin{align}\label{arma}
    \phi(B)X_t &= \theta(B)Z_t \nonumber\\
    X_t - \phi_1 X_{t-1} - \ldots - \phi_p(X_{t-p}) &= Z_t + \theta_1 Z_{t-1} + \ldots + \theta_q Z_{t-q}
\end{align}
where $\phi(B)$ and $\theta(B)$ are the AR and MA operators from (\ref{ar operator}) and (\ref{moving average operator}) with parameters $\phi_1, \ldots, \phi_p,$ $\theta_1, \ldots, \theta_q$ where $\phi_p, \theta_q \neq 0.$
}\\\\
ARMA(p, q) processes allow us to model more complex data sets while retaining the properties of the individual processes. We can rearrange (\ref{arma}) to isolate $X_t$:
\begin{equation}\label{arma isolate}
    X_t = \phi_1 X_{t-1} + \ldots + \phi_p(X_{t-p}) + Z_t + \theta_1 Z_{t-1} + \ldots + \theta_q Z_{t-q}.
\end{equation}
 These are the parameters to obtain models discussed earlier:
 \begin{enumerate}
     \item \textbf{White Noise}: ARMA(0, 0) $\Longrightarrow \phi(z)=1$ and $\theta(z)=1$
     \item \textbf{Moving Average}: ARMA(0, q) $\Longrightarrow \phi(z)=1$ and $\theta(z)=1+\theta_1 z + \ldots + \theta_q z^q$
     \item \textbf{Autoregressive}: ARMA(p, 0) $\Longrightarrow \phi(z)=1 - \phi_1z - \ldots - \phi_pz^p$ and $\theta(z)=1$
 \end{enumerate}
\subsection{Invertibility}
Let us take a closer look at (\ref{ar less 1}). We notice that it resembles an ARMA(0, $\infty$) model with $\theta_j=\phi^j$:
\begin{align}\label{arma equivalence}
    \sum_{j=0}^{\infty} \phi^j Z_{t-j} &= \sum_{j=0}^{\infty} \theta_j Z_{t-j}
\end{align}
when $\lvert \phi \rvert < 1$. Therefore, we can invert an ARMA(1, 0) process to an ARMA(0, $\infty$) process. More generally, \\\\
\textit{an ARMA(p, 0) process, $\phi(B)X_t=Z_t$, can be inverted to an ARMA(0, $\infty$) process, $X_t=\theta(B)Z_t$ if all $\lambda_i$ in 
\begin{align}\label{arma inversion}
    1-\phi_1B-\ldots-\phi_pB^p&=(1-\lambda_1B)(1-\lambda_2B)\ldots(1-\lambda_pB)\nonumber\nonumber\\
    &= \theta_1(B)\theta_2(B)\ldots \theta_p(B)
\end{align}
have magnitudes less than $1$ or lie within the complex unit circle.\\\\
}
For an ARMA(2, 0) process, we can then solve:
\begin{align}\label{arma psi derivation}
    \theta_1(B)\theta_2(B)\ldots \theta_p(B) &= (1+\lambda_1B+\lambda_1B^2+\ldots) (1+\lambda_2B+\lambda_2B^2+\ldots) \nonumber\\
    &= 1+(\lambda_1+\lambda_2)B+(\lambda_1^2+\lambda_1\lambda_2+\lambda_2^2)B^2+\ldots\nonumber\\
    &=\sum_{k=0}^{\infty}(\sum_{j=0}^k \lambda_1^j\lambda_2^{k-j})B^k\\
    &= \psi(B)\nonumber\\
\end{align}
where $\psi_k=\sum_{j=0}^k \lambda_1^j\lambda_2^{k-j}$. To derive a general method for ARMA(p, q) to ARMA(0, q) inversion, we look at (\ref{arma equivalence}). Manipulating this, we get the unique solution form of an ARMA(p, q) process:
\begin{align}\label{arma psi}
    X_t &= \frac{\theta(B)Z_t}{\phi(B)}\nonumber\\
    &= \psi(B)Z_t
\end{align}
where we have a new ARMA(0, q) process with coefficients $\psi_k$. To calculate the values of $\psi_k$, we rearrange (\ref{arma psi}) to obtain $\theta(B)=\phi(B)\psi(B)$. We can then match the corresponding coefficients for the same powers of $B$. To test for understanding, calculate the inversion for an ARMA(1, 1) process.\\
Invertible processes are useful because we can invert an MA process to an AR process to find $Z_t$, which are non-observable, using the past values of $X_t$.
\subsection{Causality}
\textit{A process has a stationary solution of the form ARMA(p, q) iff (\ref{ar poly}), $\phi(z)$, is not equal to $0$ for all $\lvert z \rvert = 1$}\\\\
Similar to an ARMA(p, 0) model, \textit{ARMA(p, q) models describe a causal process if (\ref{causal}) and its constraints hold true.}
\subsection{Redundant Parameters}
Cancelling out common factors is crucial because we might otherwise fit an ARMA(1, 1) model to white noise, or as in the following example, fit a higher order model to a simpler process.
\subsubsection{Example}
Consider a process:
\begin{align}\label{example redundant}
    X_t - 0.12X_{t-1}-0.36X_{t-2} &= Z_t - 1.6Z_{t-1}+0.6Z_{t-2}\nonumber\\
    (1-0.1B-0.42B^2)X_t &= (1+1.1.5B+0.54B^2)Z_t.
\end{align}
Naturally we think this is an ARMA(2, 2) process. However, we need to check for redundant parameters in our models. Therefore, we factor (\ref{example redundant}) the operator polynomials:
\begin{align}\label{reduced example}
    (1-0.7z)(1+0.6z) &= (1+0.9z)(1+0.6z)\nonumber\\
    (1-0.7z) &= (1+0.9z)
\end{align}
where we divided by $(1+0.6z)$, the common factor. It turns out our process is actually an ARMA(1, 1) model. \\\\
Our model is causal because:
\begin{equation}
    \phi(z) = 1-0.7z = 0 \qquad \text{when } z=10/7,
\end{equation}
which has magnitude larger than 1 or $\lambda_1 < 1$. This model is also invertible because:
\begin{equation}
    \theta(z) = 1+0.9z=0 \qquad \text{when } z=-10/9,
\end{equation}
which also has a magnitude larger than 1 or $\lambda_2 < 1$.\\\\
To find the unique solution, we must invert the ARMA(1, 1) model into an ARMA(0, $\infty$) model. Consequently, we need to calculate the coefficients $\psi_k$. For this ARMA(1, 1) process, we have:
\begin{align}\label{arma 1,1}
    1 + 0.9 z &= (1-0.7 z)(\psi_0+\psi_1 z +\ldots)\nonumber\\
    &= \psi_0 + (\psi_1-0.7\psi_0)z + (\psi_2-0.7\psi_1)z^2+\ldots\nonumber
\end{align}
Now we match coefficients for $z^k$:
\begin{align}
    1 &= \psi_0\nonumber\\
    0.9 &= \psi_1-0.7\psi_0\nonumber\\
    0&=\psi_j-0.7\psi_{j-1} \qquad \text{for } j\geq 2.\nonumber
\end{align}
We can then solve this system of equations using matrices, but we will manually solve in this example.
\begin{align}
    \psi_0 &= 1\nonumber\\
    \psi_1 &= 0.7 + 0.9=1.6\nonumber\\
    \psi_j &= (0.7)^{j-1}(0.7 + 0.9)=(0.7)^{j-1}(1.6) \qquad \text{for } j \geq2\nonumber
\end{align}
We now have the unique, stationary solution for $\{X_t\}$:
\begin{align}
    X_t &= \sum_{j=0}^{\infty} \psi_j Z_{t-j} \nonumber\\
    &=Z_t + 1.6\sum_{j=0}^{\infty}(0.7)^{j-1}Z_{t-j}.\nonumber
\end{align}
\subsection{\emph{ACVF} and \emph{ACF} of ARMA Processes}
From now on, we will use invertible, causal, and stationary ARMA processes when referring to ARMA processes. We will cover several approaches to compute the \emph{acvf} and \emph{acf} of ARMA processes.
\subsubsection{Polynomial Division}
Remember an ARMA process can be written as (\ref{arma psi}), which expands to:
\begin{equation}\label{arma psi expanded}
    X_t = \psi_0 Z_t + \psi_1 Z_{t-1} + \ldots
\end{equation}
We can directly calculate the \emph{acvf} as:
\begin{align}\label{arma acvf direct}
    K_{XX}(\tau)&=cov(X_t, X_{t+\tau})\nonumber\\
    &=\sigma_Z^2 \sum_{j=0}^{\infty} \psi_j \psi_{j+\tau} \qquad \text{for } \tau \geq 0
\end{align}
where $\sigma_Z^2$ is the variance of $Z_t$. The \emph{acf} is then:
\begin{equation}
    \rho(\tau) = \frac{K_{XX}(\tau)}{K_{XX}(0)}.
\end{equation}
%Maybe insert example if have time
\subsubsection{Difference Equations}
Difference equations represent recursive functions or processes. In fact, we have seen them before in these notes, e.g. in Yule-Walker equations. Now let us derive the \emph{acvf} for an ARMA(p, q) process. For any $k\geq0$:
\begin{align}
    \text{Cov}(\theta(B)Z_t, X_{t-k})z&=\text{Cov}(\phi(B)X_t,X_{t-k}) \label{equate covariances}\\
    &= \text{Cov}(X_t-\phi_1X_{t-1}-\ldots-\phi_pX_{t-p}, X_{t-k})\nonumber\\
    &=\text{Cov}(X_t,X_{t-k})-\phi_1\text{Cov}(X_{t-1}, X_{t-k})-\ldots-\phi_p \text{Cov}(X_{t-p}, X_{t-k})\nonumber\\
    &= K_{XX}(k) - \phi_1K_{XX}(k-1)-\ldots-\phi_p K_{XX}(k-p). \label{phi expanded equate covariance}
\end{align}
We can use (\ref{arma psi}) to expand the left hand side of (\ref{equate covariances}):
\begin{align}\label{theta expanded equate covariances}
    \text{Cov}(\phi(B)X_t,X_{t-k}) &= \text{Cov}(Z_t+\theta_1Z_{t-1} +\ldots+\theta_qZ_{t-q}, \psi_0Z_{t-k} + \psi_1Z_{t-k-1}+\ldots)\nonumber\\
    &= \begin{cases}
        (\psi_0\theta_k+\psi+\theta_{k+1} + \ldots+\psi_{q-k}\theta_q)\sigma_Z^2 \qquad &\text{if } k \leq q\\
        0 \qquad &\text{if } k > q.
    \end{cases}
\end{align}
We then plug in (\ref{phi expanded equate covariance}) and (\ref{theta expanded equate covariances}) to (\ref{equate covariances}) to get:
\begin{equation}
    K_{XX}(k)-\phi_1K_{XX}(k-1)-\ldots-\phi_pK_{XX}(k-p)=c_k \qquad \forall k \geq 0
\end{equation}
where
\begin{equation}
    c_k = \begin{cases}
        (\psi_0\theta_k+\psi+\theta_{k+1} + \ldots+\psi_{q-k}\theta_q)\sigma_Z^2 \qquad &\text{if } k \leq q\\
        0 \qquad &\text{if } k > q.
    \end{cases}
\end{equation}
Remember that $K_{XX}(-k)=K_{XX}(k)$, allowing us to build (\ref{yule-walker matrix}). A benefit of difference equations over polynomial division is that we only calculate $q$ parameters compared to $j$ parameters.
\subsection{Parameter Estimation}
We will now discuss the over-looming question at the back of our heads. How do we estimate the parameters $\theta_k$ and $\psi_k$? There are a few ways to go about it, including conditional least squares (LS), and maximum likelihood estimation (MLE). Here, we will present an algorithm for solving the parameters using conditional least squares in an iterative fashion very reminiscent of the optimization algorithm for k-means clustering, the problem in which we divide a set of points into k distinct clusters.

\subsection{Conditional Least Squares}
As described in the previous subsection, we will use conditional least squares iteratively in order to reach an optimal solution. Our iteration goes as follows:

\begin{enumerate}
    \item Given current estimations for the parameters $(\phi_{1}, \phi_{2}, ... \theta_{1}, \theta_{2})$, calculate the current residuals from the time series data.
    \item Calculate each residual as follows: \\
    $Z_{t} = X_{t} - \mu - \phi_1(X_{t-1} - \mu) - ... - \phi_{p}(X_{t-p} - \mu) - \theta_{1}(Z_{t-1}) - ... - \theta_{q}(Z{t-q})$
    \item Solve for new optimal parameters, $(\phi_{1}^*, \phi_{2}^*, ... \theta_{1}^*, \theta_{2}^*)$, conditioned on the calculated residuals and observations using least squares, stacking known values into a data matrix and then solving using typical least squares techniques.
\end{enumerate}

We see that this process can be performed iteratively to further fine tune the residuals and the parameter estimates. Therefore, our entire process begins with initializing some random weights for the $\phi$'s and $\theta$'s, and performing this process repeatedly until convergence. This process is extremely similar to K-means clustering, involving a step fixing some variables, then solving for another parameter that maximizes a value conditioned on those fixed variables. The fixing of variables in K-means clustering is the assignment of points to clusters. The fixing of variables in the ARMA parameter solving is the calculation of residuals. The maximization in K-means clustering is the assignment of cluster centers as the centroid of the points in the cluster. The maximization in ARMA parameter solving is the solving of parameters $\phi$'s and $\theta$'s given the current residuals.


\section{Extensions of ARMA}
\subsection{ARIMA}
    After deriving ARMA, auto-regressive and moving average, models, we are finally ready to extend to ARIMA. The jump between the two is actually quite small, including only one new step, differencing. 
    
    \begin{align*}
    ARIMA(p, d, q) = (1 - \phi_{1}L - \phi_{2}L^{2} - \phi_{3}L^{3}... - \phi_{p}L^{p}) (1 - L)^{d} y_{t} = c + (1 + \theta_{1} L + \theta_{2} L^{2}... + \theta_{q} L^{q}) \epsilon_{t}
    \end{align*}
    
    The purpose of differencing is to eliminate trends and enforce stationarity in the data, one of the prerequisites for the ARMA model to fit properly to the time series data. Conveniently, we can perform the differencing prior to the ARMA parameter fitting, meaning we make no changes to the estimation of the model parameters, we simply operate on new differenced data.
    
\subsection{SARIMA}
    Sometimes, our data has a seasonality, or cyclical behavior. In this case, ARIMA models are not expressive enough in order to truly model the behavior of the time series. In these instances, we add seasonal terms in order to fully express the time series model.
    
    \begin{align*}
    SARIMA(p, d, q) (P, D, Q)_{m} = (1 - \phi_{1}L - \phi_{2}L^{2} - \phi_{3}L^{3}... - \phi_{p}L^{p}) \\(1 - \Phi_{1}L^{m} - \Phi_{2}L^{2m} - \Phi_{3}L^{3m}... - \Phi_{P}L^{Pm}) (1 - L)^{d} \\(1 - L)^{Dm} y_{t} = c + (1 + \theta_{1} L + \theta_{2} L^{2}... + \theta_{q} L^{q}) \\(1 + \Theta_{1} L^{m} + \Theta_{2} L^{2m}... + \Theta_{Q} L^{Qm}) \epsilon_{t}
    \end{align*}
    
    The SARIMA terms compare current observations against observations one period ago, instead of one time step ago. This applies for both AR terms, I terms, and MA terms. The parameter estimation happens in the same fashion, just with different observations being depended on for a current observation.

\section{Model Section}
There are several methods to determine the best model among a family of alternatives, i.e. they help us choose $p, q$ that is appropriate for a data set. Like with linear regression, we can fit a polynomial of degree 19 for a training dataset of 20 points - this model will perfectly fit the training dataset but will most likely perform poorly on future values. Similar to ridge regression, in the sense of using fewer parameters, more robust models will use lower degree polynomials. Consequently, the 19-degree polynomial will most likely be more sensitive to noise than a 2-degree polynomial. Therefore, we aim to choose enough model parameters to fit the data well without losing generalization.
\subsection{Akaike Information Criterion (AIC)}
The Akaike information criterion (AIC) estimates the quality of models relative to other models of the same data set. It evaluates the log likelihood, $\ell$ of the data based on the number of parameters. The following the the likelihood function:
\begin{align}\label{mle likelihood}
    L(X_1, \ldots, X_k; \beta, \sigma^2)&=f(X_1, \ldots, X_k; \beta, \sigma^2) \nonumber\\
    &= f(X_1;\beta, \sigma^2)\prod_{i=1}^k f(X_i|X_{i-1};\beta, \sigma^2).
\end{align}
We take the $\log$ of $L$ to get:
\begin{equation}\label{log likelihood}
    \ell(\beta, \sigma^2) = \log f(X_1;\beta, \sigma^2) + \sum_{i=1}^k f(X_i|X_{i-1}; \beta, \sigma^2)
\end{equation}
\begin{equation}
    AIC = -2\log(\ell)+2k
\end{equation}
where $k=p+q+2$. The $2$ comes from the mean and noise variance. The first term measures the fit of the model, which is counteracted by $k$ because increasing the number of parameters almost always increases model fit. We choose the model that has the lowest AIC score.
\subsection{Bayesian Information Criterion (BIC)}
The Bayesion information criterion performs a similar comparison - it minimizes:
\begin{equation}
    BIC =-2\log(\ell) + k\log n
\end{equation}
where $n$ is the number of data points. We notice this penalty for more parameters is larger than the penalty of AIC. Consequently, BIC chooses sparser models compared to AIC.

\section{References}
\url{https://en.wikipedia.org/wiki/White\_noise}\\
\url{https://www.machinelearningplus.com/machine-learning/bias-variance-tradeoff/}\\
\url{https://people.duke.edu/~rnau/411diff.htm}\\
\url{https://www.stat.tamu.edu/~suhasini/teaching673/chapter3.pdf}\\
\url{http://www.maths.qmul.ac.uk/~bb/TimeSeries/TS_Chapter6_1.pdf}\\
\url{https://www.stat.berkeley.edu/~aditya/resources/LectureTWO.pdf}\\
\url{https://www.asc.ohio-state.edu/de-jong.8/note2.pdf}\\
\url{https://math.unice.fr/~frapetti/CorsoP/Chapitre_4_IMEA_1.pdf}\\
\url{https://www-jstor-org.libproxy.berkeley.edu/stable/pdf/4615673.pdf?refreqid=excelsior\%3A640726649d0edf661655f0d5d3a3e167}\\
\url{https://courses.maths.ox.ac.uk/node/view_material/924}
\end{document}